"""
Evaluation metrics for document extraction tasks
"""
import json
import re
from typing import Dict, List, Any
from collections import defaultdict


def extract_json_from_text(text: str) -> dict:
    """ä»‹æ¨¡å‹è¾“å‡ºä¸­æå–JSON,æ”¯æŒå¤šç§æ ¼å¼"""
    if not text or not isinstance(text, str):
        return {}
    
    # å¿«é€Ÿè·¯å¾„ï¼šå°è¯•ç›´æ¥è§£æ
    text_stripped = text.strip()
    if text_stripped.startswith('{') and text_stripped.endswith('}'):
        try:
            return json.loads(text_stripped)
        except:
            pass
    
    # å°è¯•æ‰¾åˆ°JSONå—ï¼ˆä»æœ€å…·ä½“åˆ°æœ€å®½æ³›ï¼‰
    patterns = [
        r'```json\s*({.*?})\s*```',  # markdown jsonå—
        r'```\s*({.*?})\s*```',       # æ™®é€šä»£ç å—
        r'({[^{}]*"relations"[^{}]*\[.*?\]\s*})',  # DocRED relationsæ ¼å¼
        r'({[^{}]*"entities"[^{}]*\[.*?\]\s*})',  # FUNSD/FinER entitiesæ ¼å¼
        r'({[^{}]*"num_items"[^{}]*})',  # CORDæ ¼å¼
        r'({.*?})',  # ä»»æ„JSONå¯¹è±¡ï¼ˆæœ€å®½æ³›ï¼‰
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, text, re.DOTALL | re.IGNORECASE)
        for match in matches:
            try:
                parsed = json.loads(match)
                # éªŒè¯æ˜¯æœ‰æ•ˆçš„å­—å…¸
                if isinstance(parsed, dict) and len(parsed) > 0:
                    return parsed
            except (json.JSONDecodeError, ValueError):
                continue
    
    # æœ€åå°è¯•ï¼šç§»é™¤å‰åé-JSONå­—ç¬¦
    try:
        start = text.find('{')
        end = text.rfind('}') + 1
        if start != -1 and end > start:
            json_str = text[start:end]
            return json.loads(json_str)
    except:
        pass
    
    return {}


# ç¼“å­˜è§„èŒƒåŒ–ç»“æœä»¥æé«˜æ€§èƒ½
_normalize_cache = {}

def normalize_entity_name(name: str) -> str:
    """æ ‡å‡†åŒ–å®ä½“åç§°ç”¨äºåŒ¹é…ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    if not name:
        return ""
    
    # æ£€æŸ¥ç¼“å­˜
    if name in _normalize_cache:
        return _normalize_cache[name]
    
    # è½¬å°å†™ï¼Œå»é™¤å¤šä½™ç©ºæ ¼ï¼Œå»é™¤ç‰¹æ®Šå­—ç¬¦
    normalized = name.lower().strip()
    normalized = re.sub(r'\s+', ' ', normalized)
    # å»é™¤å¸¸è§æ ‡ç‚¹ç¬¦å·
    normalized = re.sub(r'[.,;:!?"\']', '', normalized)
    
    # å­˜å…¥ç¼“å­˜ï¼ˆé™åˆ¶ç¼“å­˜å¤§å°ï¼‰
    if len(_normalize_cache) < 10000:
        _normalize_cache[name] = normalized
    
    return normalized


def evaluate_docred(predictions: List[Dict], golds: List[Dict], verbose: bool = False) -> Dict[str, float]:
    """
    è¯„ä¼°DocREDå…³ç³»æŠ½å–ç»“æœ
    
    Args:
        predictions: é¢„æµ‹ç»“æœåˆ—è¡¨
        golds: æ ‡å‡†ç­”æ¡ˆåˆ—è¡¨
        verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
    
    Returns:
        åŒ…å«P/R/F1ç­‰æŒ‡æ ‡çš„å­—å…¸
    """
    if len(predictions) != len(golds):
        print(f"[Warning] Length mismatch: {len(predictions)} predictions vs {len(golds)} golds")
        min_len = min(len(predictions), len(golds))
        predictions = predictions[:min_len]
        golds = golds[:min_len]
    
    if len(predictions) == 0:
        return {"precision": 0.0, "recall": 0.0, "f1": 0.0, "total_samples": 0}
    
    total_pred = 0
    total_gold = 0
    total_correct = 0
    
    # Per-relationç»Ÿè®¡
    relation_stats = defaultdict(lambda: {"pred": 0, "gold": 0, "correct": 0})
    
    parse_errors = 0
    empty_predictions = 0
    
    for idx, (pred_dict, gold_dict) in enumerate(zip(predictions, golds)):
        try:
            # è§£æé¢„æµ‹
            if isinstance(pred_dict, dict) and "prediction" in pred_dict:
                pred_text = pred_dict["prediction"]
            elif isinstance(pred_dict, str):
                pred_text = pred_dict
            else:
                pred_text = str(pred_dict)
            
            pred_json = extract_json_from_text(pred_text)
            pred_relations = pred_json.get("relations", [])
            
            if not pred_relations:
                empty_predictions += 1
            
            # è§£ææ ‡å‡†ç­”æ¡ˆ
            if isinstance(gold_dict, dict) and "gold" in gold_dict:
                gold_text = gold_dict["gold"]
            elif isinstance(gold_dict, str):
                gold_text = gold_dict
            else:
                gold_text = str(gold_dict)
            
            gold_json = extract_json_from_text(gold_text)
            gold_relations = gold_json.get("relations", [])
            
        except Exception as e:
            if verbose:
                print(f"[Error] Sample {idx} parsing failed: {e}")
            parse_errors += 1
            continue
        return {"precision": 0.0, "recall": 0.0, "f1": 0.0, "total_samples": 0}
    """
    è¯„ä¼° DocRED å…³ç³»æŠ½å–
    
    Metrics: Precision, Recall, F1 for relation triplets
    æ”¯æŒ:
    1. å®ä½“åç§°æ¨¡ç³ŠåŒ¹é…(å¿½ç•¥å¤§å°å†™)
    2. ä»æ¨¡å‹è¾“å‡ºä¸­æ™ºèƒ½æå–JSON
    3. åˆ†åˆ«ç»Ÿè®¡æ¯ä¸ªå…³ç³»ç±»å‹çš„æ€§èƒ½
    """
    pred_relations = []
    gold_relations = []
    
    # æŒ‰å…³ç³»ç±»å‹ç»Ÿè®¡
    relation_stats = defaultdict(lambda: {"tp": 0, "fp": 0, "fn": 0})
    
    for pred, gold in zip(predictions, golds):
        # è§£æé¢„æµ‹ç»“æœ
        try:
            pred_text = pred.get("prediction", "")
            pred_data = extract_json_from_text(pred_text)
            pred_rels = pred_data.get("relations", [])
            
            for r in pred_rels:
                head = normalize_entity_name(r.get("head", ""))
                tail = normalize_entity_name(r.get("tail", ""))
                rel = r.get("relation", "").strip()
                if head and tail and rel:
                    pred_relations.append((head, rel, tail))
        except Exception as e:
            pass
        
        # è§£æé‡‘æ ‡å‡†
        try:
            gold_text = gold.get("gold", gold) if isinstance(gold, dict) else gold
            gold_data = json.loads(gold_text) if isinstance(gold_text, str) else gold_text
            gold_rels = gold_data.get("relations", []) if isinstance(gold_data, dict) else []
            
            for r in gold_rels:
                head = normalize_entity_name(r.get("head", ""))
                tail = normalize_entity_name(r.get("tail", ""))
                rel = r.get("relation", "").strip()
                if head and tail and rel:
                    gold_relations.append((head, rel, tail))
        except Exception as e:
            pass
    
    pred_set = set(pred_relations)
    gold_set = set(gold_relations)
    
    # è®¡ç®—æ•´ä½“æŒ‡æ ‡
    tp = len(pred_set & gold_set)
    fp = len(pred_set - gold_set)
    fn = len(gold_set - pred_set)
    
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
    
    # æŒ‰å…³ç³»ç±»å‹ç»Ÿè®¡
    for rel_tuple in pred_set & gold_set:
        relation_stats[rel_tuple[1]]["tp"] += 1
    for rel_tuple in pred_set - gold_set:
        relation_stats[rel_tuple[1]]["fp"] += 1
    for rel_tuple in gold_set - pred_set:
        relation_stats[rel_tuple[1]]["fn"] += 1
    
    # è®¡ç®—æ¯ä¸ªå…³ç³»ç±»å‹çš„F1
    per_relation_f1 = {}
    for rel, stats in relation_stats.items():
        r_tp, r_fp, r_fn = stats["tp"], stats["fp"], stats["fn"]
        r_prec = r_tp / (r_tp + r_fp) if (r_tp + r_fp) > 0 else 0.0
        r_rec = r_tp / (r_tp + r_fn) if (r_tp + r_fn) > 0 else 0.0
        r_f1 = 2 * r_prec * r_rec / (r_prec + r_rec) if (r_prec + r_rec) > 0 else 0.0
        per_relation_f1[rel] = {"precision": r_prec, "recall": r_rec, "f1": r_f1}
    
    return {
        "precision": precision,
        "recall": recall,
        "f1": f1,
        "true_positives": tp,
        "false_positives": fp,
        "false_negatives": fn,
        "pred_count": len(pred_set),
        "gold_count": len(gold_set),
        "per_relation": per_relation_f1,
        "unique_relations_predicted": len(set(r[1] for r in pred_set)),
        "unique_relations_gold": len(set(r[1] for r in gold_set)),
    }


def evaluate_cord(predictions: List[Dict], golds: List[Dict]) -> Dict[str, float]:
    """
    è¯„ä¼° CORD æ”¶æ®æŠ½å–ï¼ˆå®˜æ–¹æ ¼å¼ï¼‰
    
    Metrics: Field-level accuracy for num_items, subtotal_price, service_price, tax_price, total_price, etc
    """
    correct_fields = defaultdict(int)
    total_fields = defaultdict(int)
    
    # Official CORD fields
    eval_fields = ["num_items", "subtotal_price", "service_price", "tax_price", "total_price", "etc"]
    
    for pred, gold in zip(predictions, golds):
        try:
            pred_data = json.loads(pred.get("prediction", "{}"))
            gold_data = json.loads(gold) if isinstance(gold, str) else gold
        except:
            continue
        
        # è¯„ä¼°æ¯ä¸ªå­—æ®µ
        for field in eval_fields:
            pred_val = str(pred_data.get(field, "")).strip()
            gold_val = str(gold_data.get(field, "")).strip()
            
            if pred_val and gold_val:  # Both have value
                if pred_val == gold_val:
                    correct_fields[field] += 1
                total_fields[field] += 1
            elif not gold_val:  # Gold doesn't have this field, skip
                continue
            else:  # Pred missing but gold has value
                total_fields[field] += 1
    
    # è®¡ç®—å‡†ç¡®ç‡
    results = {}
    for field in eval_fields:
        if total_fields[field] > 0:
            accuracy = correct_fields[field] / total_fields[field]
            results[f"{field}_accuracy"] = accuracy
    
    # æ€»ä½“å‡†ç¡®ç‡
    total_correct = sum(correct_fields.values())
    total_count = sum(total_fields.values())
    results["overall_accuracy"] = total_correct / total_count if total_count > 0 else 0.0
    
    return results


def evaluate_funsd(predictions: List[Dict], golds: List[Dict]) -> Dict[str, float]:
    """
    è¯„ä¼° FUNSD è¡¨å•ç†è§£
    
    Metrics: Entity-level and Relation-level F1
    """
    # Entity evaluation
    pred_entities = []
    gold_entities = []
    
    # Relation evaluation
    pred_relations = []
    gold_relations = []
    
    for pred, gold in zip(predictions, golds):
        try:
            # Handle prediction - may be string or dict
            if isinstance(pred, dict):
                pred_str = pred.get("prediction", "{}")
            else:
                pred_str = str(pred) if pred else "{}"
            pred_data = json.loads(pred_str) if isinstance(pred_str, str) else pred_str
            
            # Handle gold - may be string or dict
            gold_data = json.loads(gold) if isinstance(gold, str) else gold
            
            # Ensure both are dicts
            if not isinstance(pred_data, dict):
                pred_data = {}
            if not isinstance(gold_data, dict):
                gold_data = {}
        except Exception as e:
            print(f"[Warning] Failed to parse prediction/gold: {e}")
            pred_data = {}
            gold_data = {}
            continue
        
        # Extract entities
        pred_ents = pred_data.get("entities", [])
        gold_ents = gold_data.get("entities", [])
        
        pred_entities.extend([(e.get("text", ""), e.get("label", "")) for e in pred_ents])
        gold_entities.extend([(e.get("text", ""), e.get("label", "")) for e in gold_ents])
        
        # Extract relations
        pred_rels = pred_data.get("relations", [])
        gold_rels = gold_data.get("relations", [])
        
        pred_relations.extend([(r.get("head", ""), r.get("tail", ""), r.get("type", "")) for r in pred_rels])
        gold_relations.extend([(r.get("head", ""), r.get("tail", ""), r.get("type", "")) for r in gold_rels])
    
    # Calculate entity metrics
    pred_ent_set = set(pred_entities)
    gold_ent_set = set(gold_entities)
    
    ent_tp = len(pred_ent_set & gold_ent_set)
    ent_fp = len(pred_ent_set - gold_ent_set)
    ent_fn = len(gold_ent_set - pred_ent_set)
    
    ent_precision = ent_tp / (ent_tp + ent_fp) if (ent_tp + ent_fp) > 0 else 0.0
    ent_recall = ent_tp / (ent_tp + ent_fn) if (ent_tp + ent_fn) > 0 else 0.0
    ent_f1 = 2 * ent_precision * ent_recall / (ent_precision + ent_recall) if (ent_precision + ent_recall) > 0 else 0.0
    
    # Calculate relation metrics
    pred_rel_set = set(pred_relations)
    gold_rel_set = set(gold_relations)
    
    rel_tp = len(pred_rel_set & gold_rel_set)
    rel_fp = len(pred_rel_set - gold_rel_set)
    rel_fn = len(gold_rel_set - pred_rel_set)
    
    rel_precision = rel_tp / (rel_tp + rel_fp) if (rel_tp + rel_fp) > 0 else 0.0
    rel_recall = rel_tp / (rel_tp + rel_fn) if (rel_tp + rel_fn) > 0 else 0.0
    rel_f1 = 2 * rel_precision * rel_recall / (rel_precision + rel_recall) if (rel_precision + rel_recall) > 0 else 0.0
    
    return {
        "entity_precision": ent_precision,
        "entity_recall": ent_recall,
        "entity_f1": ent_f1,
        "relation_precision": rel_precision,
        "relation_recall": rel_recall,
        "relation_f1": rel_f1,
        "overall_f1": (ent_f1 + rel_f1) / 2
    }


def evaluate_finer(predictions: List[Dict], golds: List[Dict]) -> Dict[str, float]:
    """
    è¯„ä¼° FinER-139 é‡‘èå®ä½“è¯†åˆ«
    
    Metrics: Entity-level Precision, Recall, F1
    """
    pred_entities = []
    gold_entities = []
    
    for pred, gold in zip(predictions, golds):
        try:
            pred_data = json.loads(pred.get("prediction", "{}"))
            gold_data = json.loads(gold) if isinstance(gold, str) else gold
        except:
            continue
        
        pred_ents = pred_data.get("entities", [])
        gold_ents = gold_data.get("entities", [])
        
        # ä½¿ç”¨ (text, label) ä½œä¸ºå”¯ä¸€æ ‡è¯†
        pred_entities.extend([(e.get("text", ""), e.get("label", "")) for e in pred_ents])
        gold_entities.extend([(e.get("text", ""), e.get("label", "")) for e in gold_ents])
    
    pred_set = set(pred_entities)
    gold_set = set(gold_entities)
    
    tp = len(pred_set & gold_set)
    fp = len(pred_set - gold_set)
    fn = len(gold_set - pred_set)
    
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
    
    return {
        "precision": precision,
        "recall": recall,
        "f1": f1,
        "true_positives": tp,
        "false_positives": fp,
        "false_negatives": fn
    }


def evaluate_extraction_task(task: str, predictions: List[Dict], golds: List[Dict]) -> Dict[str, float]:
    """
    ç»Ÿä¸€è¯„ä¼°æ¥å£
    
    Args:
        task: 'docred', 'cord', 'funsd', 'finer'
        predictions: é¢„æµ‹ç»“æœåˆ—è¡¨
        golds: é‡‘æ ‡å‡†åˆ—è¡¨
    
    Returns:
        è¯„ä¼°æŒ‡æ ‡å­—å…¸
    """
    if task == "docred":
        return evaluate_docred(predictions, golds)
    elif task == "cord":
        return evaluate_cord(predictions, golds)
    elif task == "funsd":
        return evaluate_funsd(predictions, golds)
    elif task == "finer":
        return evaluate_finer(predictions, golds)
    else:
        return {"error": f"Unknown task: {task}"}


def print_evaluation_results(task: str, metrics: Dict[str, float]):
    """
    æ‰“å°è¯„ä¼°ç»“æœ
    """
    print("\n" + "="*60)
    print(f"ğŸ“Š Evaluation Results for {task.upper()}")
    print("="*60)
    
    for metric, value in metrics.items():
        if isinstance(value, float):
            print(f"  {metric:40s}: {value:6.2%}")
        else:
            print(f"  {metric:40s}: {value}")
    
    print("="*60 + "\n")


if __name__ == "__main__":
    # æµ‹è¯•ç¤ºä¾‹
    test_predictions = [
        {"prediction": '{"relations": [{"head": "Apple", "relation": "founded_by", "tail": "Steve Jobs"}]}'}
    ]
    test_golds = [
        [{"head": "Apple", "relation": "founded_by", "tail": "Steve Jobs"}]
    ]
    
    metrics = evaluate_docred(test_predictions, test_golds)
    print_evaluation_results("docred", metrics)
