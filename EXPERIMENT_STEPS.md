# LatentMAS + LoRA å®Œæ•´å®éªŒæ­¥éª¤

## âš ï¸ é‡è¦æ›´æ–° (2026-01-10)

### ä»£ç æ”¹è¿›è¯´æ˜

æœ¬é¡¹ç›®å·²å®Œæˆä»¥ä¸‹å…³é”®æ”¹è¿›ï¼Œç¡®ä¿LoRAè®­ç»ƒèƒ½çœŸæ­£å­¦ä¹ åˆ°å…³ç³»æŠ½å–èƒ½åŠ›ï¼š

**1. è®­ç»ƒæ ¼å¼æ”¹è¿› (finetune_lora.py)**
- DocREDè®­ç»ƒpromptç°åœ¨åŒ…å«å®Œæ•´çš„entity_listï¼ˆå®ä½“åˆ—è¡¨ï¼‰
- æ˜ç¡®è¯´æ˜å¸¸ç”¨å…³ç³»ç±»å‹ï¼ˆP17, P131, P27ç­‰ï¼‰åŠå…¶å«ä¹‰
- è¾“å‡ºæ ¼å¼è¯´æ˜ä¸æ¨ç†æ—¶å®Œå…¨ä¸€è‡´
- æ–°å¢evidenceå­—æ®µè®­ç»ƒï¼ˆå¥å­ç´¢å¼•åˆ—è¡¨ï¼‰

**2. æ¨ç†æµç¨‹ä¼˜åŒ– (methods/latent_mas.py)**
- LoRAæ¨¡å‹ä½¿ç”¨ç›´æ¥æ¨ç†æ¨¡å¼ï¼Œè·³è¿‡planner/critic/refiner/judgerå¤šagentæµç¨‹
- æ¨ç†promptä¸è®­ç»ƒpromptå®Œå…¨ä¸€è‡´ï¼Œç¡®ä¿æ ¼å¼åŒ¹é…
- æ–°å¢ `build_lora_extraction_prompt()` ä¸“ç”¨å‡½æ•°

**3. è¯„ä¼°æ”¹è¿› (evaluate_extraction.py)**
- æ™ºèƒ½JSONæå–ï¼šæ”¯æŒä»å„ç§æ ¼å¼çš„æ¨¡å‹è¾“å‡ºä¸­æå–JSON
- å®ä½“åç§°æ¨¡ç³ŠåŒ¹é…ï¼šå¿½ç•¥å¤§å°å†™å·®å¼‚
- åˆ†å…³ç³»ç±»å‹ç»Ÿè®¡ï¼šè¾“å‡ºæ¯ä¸ªP-IDçš„Precision/Recall/F1
- æ›´è¯¦ç»†çš„é”™è¯¯åˆ†æ

**4. æ•°æ®æ ¼å¼ç®€åŒ– (data.py)**
- goldè¾“å‡ºåªåŒ…å«`{"relations": [...]}`ï¼Œä¸å†åŒ…å«raw_labels
- ä¿ç•™raw_labelsç”¨äºå®˜æ–¹è¯„ä¼°æ ¼å¼è½¬æ¢

---

## å‰ç½®å‡†å¤‡

### 1. æ£€æŸ¥ç¯å¢ƒ
```powershell
# æ£€æŸ¥CUDAå’ŒGPU
nvidia-smi

# æ£€æŸ¥Pythonç‰ˆæœ¬ï¼ˆéœ€è¦3.8+ï¼‰
python --version

# æ£€æŸ¥ä¾èµ–åŒ…
pip list | Select-String "torch|transformers|peft"
```

### 2. å®‰è£…ä¾èµ–ï¼ˆå¦‚æœè¿˜æ²¡å®‰è£…ï¼‰
```powershell
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install transformers==4.50.0
pip install peft==0.14.0
pip install accelerate bitsandbytes
pip install qwen-vl-utils
pip install pillow opencv-python
```

### 3. å‡†å¤‡æ•°æ®é›†

**FUNSDï¼ˆè¡¨å•ç†è§£ï¼‰**
```
data/funsd/
  â”œâ”€â”€ instances_train.json    # COCOæ ¼å¼è®­ç»ƒæ•°æ®
  â”œâ”€â”€ test.json               # æµ‹è¯•æ•°æ®
  â”œâ”€â”€ annotations/            # åŸå§‹æ ‡æ³¨
  â””â”€â”€ imgs/                   # å›¾åƒæ–‡ä»¶
```

**DocREDï¼ˆå…³ç³»æŠ½å–ï¼‰**
```
data/
  â”œâ”€â”€ test_docred.json        # æµ‹è¯•æ•°æ®
  â””â”€â”€ docred/
      â””â”€â”€ train_annotated.json  # è®­ç»ƒæ•°æ®
```

---

## å®éªŒä¸€ï¼šFUNSDåŸºçº¿æµ‹è¯•ï¼ˆæ— LoRAï¼‰

### æ­¥éª¤1ï¼šè¿è¡ŒåŸºçº¿æ¨¡å‹
```powershell
# ä½¿ç”¨50ä¸ªæ ·æœ¬å¿«é€Ÿæµ‹è¯•
python run.py `
    --task funsd `
    --model_name Qwen/Qwen3-VL-4B-Instruct `
    --method latent_mas `
    --architecture hierarchical `
    --doc_path data/test.json `
    --annotations_dir data/funsd/annotations `
    --image_dir data/funsd/imgs `
    --max_samples 50 `
    --batch_size 1 `
    --output_path results/funsd_baseline_50samples.json

# è®°å½•ç»“æœï¼šEntity F1, Relation F1, è¾“å‡ºæ ¼å¼æ­£ç¡®ç‡
```

**é¢„æœŸç»“æœï¼š**
- Entity F1: ~30-40%
- Relation F1: ~10-20%
- å¸¸è§é—®é¢˜ï¼šè¾“å‡ºæ ¼å¼ä¸è§„èŒƒã€å…³ç³»è¯†åˆ«ä¸å‡†ç¡®

---

## å®éªŒäºŒï¼šFUNSD LoRAå¾®è°ƒï¼ˆå°è§„æ¨¡å¿«é€ŸéªŒè¯ï¼‰

### æ­¥éª¤1ï¼šä½¿ç”¨100ä¸ªæ ·æœ¬è®­ç»ƒLoRA
```powershell
python finetune_lora.py `
    --model_name Qwen/Qwen3-VL-4B-Instruct `
    --task funsd `
    --train_data data/funsd/instances_train.json `
    --annotations_dir data/funsd/annotations `
    --image_dir data/funsd/imgs `
    --output_dir lora_weights/funsd_100samples `
    --max_train_samples 100 `
    --epochs 5 `
    --batch_size 2 `
    --gradient_accumulation_steps 8 `
    --learning_rate 2e-4 `
    --lora_r 16 `
    --lora_alpha 32

# é¢„è®¡ç”¨æ—¶ï¼š30-60åˆ†é’Ÿï¼ˆå–å†³äºGPUï¼‰
```

### æ­¥éª¤2ï¼šæµ‹è¯•LoRAæ¨¡å‹
```powershell
python run.py `
    --task funsd `
    --model_name Qwen/Qwen3-VL-4B-Instruct `
    --lora_weights lora_weights/funsd_100samples `
    --method latent_mas `
    --architecture hierarchical `
    --doc_path data/test.json `
    --annotations_dir data/funsd/annotations `
    --image_dir data/funsd/imgs `
    --max_samples 50 `
    --batch_size 1 `
    --output_path results/funsd_lora100_50samples.json
```

### æ­¥éª¤3ï¼šå¯¹æ¯”ç»“æœ
```powershell
# å¯¹æ¯”baselineå’ŒLoRAç»“æœ
python evaluate_extraction.py `
    --pred_file results/funsd_baseline_50samples.json `
    --gold_file data/test.json

python evaluate_extraction.py `
    --pred_file results/funsd_lora100_50samples.json `
    --gold_file data/test.json

# é¢„æœŸæå‡ï¼šEntity F1 +5-10%, Relation F1 +10-15%
```

---

## å®éªŒä¸‰ï¼šFUNSDå®Œæ•´è®­ç»ƒï¼ˆå…¨éƒ¨æ•°æ®ï¼‰

### æ­¥éª¤1ï¼šä½¿ç”¨å…¨éƒ¨è®­ç»ƒæ•°æ®å¾®è°ƒ
```powershell
# ä½¿ç”¨4å¼ GPUå¹¶è¡Œè®­ç»ƒï¼ˆå¦‚æœåªæœ‰1å¼ GPUï¼Œå»æ‰CUDA_VISIBLE_DEVICESï¼‰
$env:CUDA_VISIBLE_DEVICES="0,1,2,3"
python finetune_lora.py `
    --model_name Qwen/Qwen3-VL-4B-Instruct `
    --task funsd `
    --train_data data/funsd/instances_train.json `
    --annotations_dir data/funsd/annotations `
    --image_dir data/funsd/imgs `
    --output_dir lora_weights/funsd_full `
    --epochs 3 `
    --batch_size 2 `
    --gradient_accumulation_steps 8 `
    --learning_rate 2e-4

# é¢„è®¡ç”¨æ—¶ï¼š2-4å°æ—¶
```

### æ­¥éª¤2ï¼šå®Œæ•´æµ‹è¯•é›†è¯„ä¼°
```powershell
python run.py `
    --task funsd `
    --model_name Qwen/Qwen3-VL-4B-Instruct `
    --lora_weights lora_weights/funsd_full `
    --method latent_mas `
    --architecture hierarchical `
    --doc_path data/test.json `
    --annotations_dir data/funsd/annotations `
    --image_dir data/funsd/imgs `
    --output_path results/funsd_lora_full.json

# è¯„ä¼°
python evaluate_extraction.py `
    --pred_file results/funsd_lora_full.json `
    --gold_file data/test.json
```

---

## å®éªŒå››ï¼šDocREDå…³ç³»æŠ½å–ï¼ˆæ–‡æœ¬æ¨¡å‹ï¼‰

### æ­¥éª¤1ï¼šåŸºçº¿æµ‹è¯•
```powershell
python run.py `
    --task docred `
    --model_name Qwen/Qwen2.5-7B-Instruct `
    --method latent_mas `
    --architecture hierarchical `
    --doc_path data/test_docred.json `
    --max_samples 50 `
    --output_path results/docred_baseline_50samples.json
```

### æ­¥éª¤2ï¼šLoRAå¾®è°ƒï¼ˆä½¿ç”¨500ä¸ªæ ·æœ¬ï¼‰
```powershell
python finetune_lora.py `
    --model_name Qwen/Qwen2.5-7B-Instruct `
    --task docred `
    --train_data data/docred/train_annotated.json `
    --output_dir lora_weights/docred_500samples `
    --max_train_samples 500 `
    --epochs 5 `
    --batch_size 4 `
    --gradient_accumulation_steps 4 `
    --learning_rate 1e-4

# é¢„è®¡ç”¨æ—¶ï¼š1-2å°æ—¶
```

### æ­¥éª¤3ï¼šæµ‹è¯•LoRAæ•ˆæœ
```powershell
python run.py `
    --task docred `
    --model_name Qwen/Qwen2.5-7B-Instruct `
    --lora_weights lora_weights/docred_500samples `
    --method latent_mas `
    --architecture hierarchical `
    --doc_path data/test_docred.json `
    --max_samples 50 `
    --output_path results/docred_lora500_50samples.json

# è¯„ä¼°
python evaluate_extraction.py `
    --pred_file results/docred_lora500_50samples.json `
    --gold_file data/test_docred.json
```

---

## å®éªŒäº”ï¼šæ¶ˆèå®éªŒï¼ˆæ•°æ®é‡å¯¹æ¯”ï¼‰

### æµ‹è¯•ä¸åŒè®­ç»ƒæ ·æœ¬æ•°çš„å½±å“

```powershell
# 100ä¸ªæ ·æœ¬
python finetune_lora.py --task funsd --max_train_samples 100 --output_dir lora_weights/funsd_100 --epochs 5 ...

# 300ä¸ªæ ·æœ¬
python finetune_lora.py --task funsd --max_train_samples 300 --output_dir lora_weights/funsd_300 --epochs 4 ...

# 500ä¸ªæ ·æœ¬
python finetune_lora.py --task funsd --max_train_samples 500 --output_dir lora_weights/funsd_500 --epochs 3 ...

# å…¨éƒ¨æ ·æœ¬
python finetune_lora.py --task funsd --output_dir lora_weights/funsd_full --epochs 3 ...

# åˆ†åˆ«æµ‹è¯•å’Œå¯¹æ¯”ç»“æœ
```

---

## å®éªŒå…­ï¼šæ¶æ„å¯¹æ¯”ï¼ˆSequential vs Hierarchicalï¼‰

```powershell
# Sequentialæ¶æ„ + LoRA
python run.py `
    --task funsd `
    --model_name Qwen/Qwen3-VL-4B-Instruct `
    --lora_weights lora_weights/funsd_full `
    --method latent_mas `
    --architecture sequential `
    --doc_path data/test.json `
    --annotations_dir data/funsd/annotations `
    --image_dir data/funsd/imgs `
    --output_path results/funsd_lora_sequential.json

# Hierarchicalæ¶æ„ + LoRA
python run.py `
    --task funsd `
    --model_name Qwen/Qwen3-VL-4B-Instruct `
    --lora_weights lora_weights/funsd_full `
    --method latent_mas `
    --architecture hierarchical `
    --doc_path data/test.json `
    --annotations_dir data/funsd/annotations `
    --image_dir data/funsd/imgs `
    --output_path results/funsd_lora_hierarchical.json

# å¯¹æ¯”å“ªç§æ¶æ„æ•ˆæœæ›´å¥½
```

---

## å®éªŒä¸ƒï¼šè¶…å‚æ•°è°ƒä¼˜

### æµ‹è¯•ä¸åŒLoRAé…ç½®

```powershell
# é…ç½®1ï¼šå°rankï¼ˆr=8ï¼‰
python finetune_lora.py --task funsd --lora_r 8 --lora_alpha 16 --output_dir lora_weights/funsd_r8 ...

# é…ç½®2ï¼šä¸­rankï¼ˆr=16ï¼Œé»˜è®¤ï¼‰
python finetune_lora.py --task funsd --lora_r 16 --lora_alpha 32 --output_dir lora_weights/funsd_r16 ...

# é…ç½®3ï¼šå¤§rankï¼ˆr=32ï¼‰
python finetune_lora.py --task funsd --lora_r 32 --lora_alpha 64 --output_dir lora_weights/funsd_r32 ...

# å¯¹æ¯”ï¼šå‚æ•°é‡ vs æ€§èƒ½ vs è®­ç»ƒæ—¶é—´
```

---

## å¿«é€Ÿè¯Šæ–­å‘½ä»¤

### æ£€æŸ¥è®­ç»ƒæ—¥å¿—
```powershell
# å®æ—¶ç›‘æ§è®­ç»ƒ
Get-Content lora_weights/funsd_full/trainer_log.txt -Wait

# æ£€æŸ¥æœ€åå‡ è¡Œ
Get-Content lora_weights/funsd_full/trainer_log.txt -Tail 20
```

### æ£€æŸ¥LoRAæƒé‡æ˜¯å¦ä¿å­˜
```powershell
ls lora_weights/funsd_full/
# åº”è¯¥çœ‹åˆ°ï¼šadapter_config.json, adapter_model.safetensors
```

### å¿«é€ŸéªŒè¯å•ä¸ªæ ·æœ¬
```powershell
python run.py `
    --task funsd `
    --model_name Qwen/Qwen3-VL-4B-Instruct `
    --lora_weights lora_weights/funsd_100samples `
    --doc_path data/test.json `
    --annotations_dir data/funsd/annotations `
    --image_dir data/funsd/imgs `
    --max_samples 1 `
    --output_path test_single.json
```

---

## ç»“æœè®°å½•æ¨¡æ¿

åˆ›å»º `experiment_results.txt` è®°å½•æ¯æ¬¡å®éªŒï¼š

```
å®éªŒæ—¥æœŸï¼š2026-01-09
ä»»åŠ¡ï¼šFUNSD
æ¨¡å‹ï¼šQwen3-VL-4B-Instruct
é…ç½®ï¼š
  - è®­ç»ƒæ ·æœ¬ï¼š100
  - Epochsï¼š5
  - Batch sizeï¼š2
  - LoRA rï¼š16
  - Learning rateï¼š2e-4

ç»“æœï¼š
  - Entity F1ï¼š45.2%
  - Relation F1ï¼š32.8%
  - è®­ç»ƒæ—¶é—´ï¼š45åˆ†é’Ÿ
  - GPUæ˜¾å­˜ï¼š18GB

å¯¹æ¯”åŸºçº¿ï¼š
  - Entity F1æå‡ï¼š+8.5%
  - Relation F1æå‡ï¼š+15.3%

å¤‡æ³¨ï¼šæ ¼å¼è¾“å‡ºæ˜æ˜¾æ”¹å–„ï¼Œå…³ç³»è¯†åˆ«æ›´å‡†ç¡®
```

---

## å¸¸è§é—®é¢˜æ’æŸ¥

### é—®é¢˜1ï¼šCUDA out of memory
```powershell
# è§£å†³æ–¹æ¡ˆï¼šå‡å°batch sizeæˆ–ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
--batch_size 1 --gradient_accumulation_steps 16
```

### é—®é¢˜2ï¼šå›¾åƒåŠ è½½å¤±è´¥
```powershell
# æ£€æŸ¥å›¾åƒè·¯å¾„
ls data/funsd/imgs/

# ç¡®è®¤instances_train.jsonä¸­çš„file_nameå­—æ®µæ­£ç¡®
```

### é—®é¢˜3ï¼šè®­ç»ƒlossä¸ä¸‹é™
```powershell
# å°è¯•è°ƒæ•´å­¦ä¹ ç‡
--learning_rate 1e-4  # é™ä½å­¦ä¹ ç‡
--learning_rate 5e-4  # æé«˜å­¦ä¹ ç‡
```

### é—®é¢˜4ï¼šæ¨ç†æ—¶æ‰¾ä¸åˆ°LoRAæƒé‡
```powershell
# æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
ls lora_weights/funsd_full/adapter_model.safetensors

# ç¡®è®¤è·¯å¾„æ­£ç¡®
--lora_weights lora_weights/funsd_full  # ä¸è¦åŒ…å«æ–‡ä»¶å
```

### é—®é¢˜5ï¼šbf16ä¸æ”¯æŒé”™è¯¯
**é”™è¯¯ä¿¡æ¯**ï¼š`ValueError: Your setup doesn't support bf16/gpu`

**åŸå› **ï¼šGPUä¸æ”¯æŒbf16ï¼ˆéœ€è¦Ampereæ¶æ„æˆ–æ›´æ–°ï¼Œå¦‚RTX 30ç³»åˆ—ã€A100ç­‰ï¼‰

**è§£å†³æ–¹æ¡ˆ**ï¼šä»£ç å·²è‡ªåŠ¨æ£€æµ‹GPUèƒ½åŠ›ï¼Œä¼šè‡ªåŠ¨é™çº§åˆ°fp16
- AmpereåŠä»¥ä¸Šï¼ˆRTX 30/40ç³»åˆ—ã€A100/H100ï¼‰ï¼šä½¿ç”¨bf16
- è¾ƒè€GPUï¼ˆRTX 20ç³»åˆ—ã€V100ã€GTX 1080ç­‰ï¼‰ï¼šè‡ªåŠ¨ä½¿ç”¨fp16

**æ‰‹åŠ¨æ£€æŸ¥GPUèƒ½åŠ›**ï¼š
```python
import torch
print(torch.cuda.get_device_capability())  # (8, 0)åŠä»¥ä¸Šæ”¯æŒbf16
```

### é—®é¢˜6ï¼šè®­ç»ƒæ—¶æ²¡æœ‰æ¢¯åº¦ï¼ˆQwen3æ¨¡å‹ï¼‰
**é”™è¯¯ä¿¡æ¯**ï¼š`RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn`

**åŸå› 1**ï¼šQwen3æ¨¡å‹çš„å±‚å‘½åå¯èƒ½ä¸ç¡¬ç¼–ç çš„`target_modules`ä¸åŒ¹é…ï¼Œå¯¼è‡´LoRAæ²¡æœ‰åº”ç”¨åˆ°ä»»ä½•å±‚

**åŸå› 2**ï¼š`gradient_checkpointing=True` ä¸PEFT LoRAåœ¨æŸäº›é…ç½®ä¸‹ä¸å…¼å®¹

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. ä»£ç å·²æ›´æ–°ä¸º**è‡ªåŠ¨æ£€æµ‹æ¨¡å‹çš„Linearå±‚**ï¼Œé€‚é…æ‰€æœ‰Qwenç³»åˆ—æ¨¡å‹
2. ç¦ç”¨gradient_checkpointingï¼ˆè™½ç„¶ä¼šå¢åŠ æ˜¾å­˜ä½¿ç”¨ï¼Œä½†ä¿è¯è®­ç»ƒç¨³å®šï¼‰
3. æ˜¾å¼å¯ç”¨LoRAå‚æ•°çš„æ¢¯åº¦

**éªŒè¯LoRAæ˜¯å¦æ­£ç¡®åº”ç”¨**ï¼š
è®­ç»ƒå¼€å§‹æ—¶ä¼šæ‰“å°ï¼š
```
LoRA target modules: ['q_proj', 'k_proj', 'v_proj', ...]
trainable params: 43,646,976 || all params: 8,234,382,336 || trainable%: 0.5301
```
ç¡®ä¿trainable%å¤§äº0ï¼

**å¦‚æœæ˜¾å­˜ä¸è¶³**ï¼š
```powershell
# å‡å°batch sizeå¹¶å¢åŠ æ¢¯åº¦ç´¯ç§¯
--batch_size 1 --gradient_accumulation_steps 16

# æˆ–å‡å°LoRA rank
--lora_r 8 --lora_alpha 16
```

---

## æ¨èå®éªŒæµç¨‹ï¼ˆæ–°æ‰‹ï¼‰

1. **Day 1ï¼šç¯å¢ƒå‡†å¤‡** âœ…
   - å®‰è£…ä¾èµ–
   - å‡†å¤‡æ•°æ®
   - è¿è¡ŒåŸºçº¿ï¼ˆ10ä¸ªæ ·æœ¬å¿«é€Ÿæµ‹è¯•ï¼‰

2. **Day 2ï¼šå¿«é€ŸéªŒè¯** ğŸš€
   - è®­ç»ƒ100æ ·æœ¬LoRAï¼ˆ1å°æ—¶ï¼‰
   - æµ‹è¯•æ•ˆæœ
   - è°ƒè¯•é—®é¢˜

3. **Day 3ï¼šå®Œæ•´è®­ç»ƒ** ğŸ¯
   - è®­ç»ƒå…¨éƒ¨æ•°æ®LoRAï¼ˆ3-4å°æ—¶ï¼‰
   - å®Œæ•´è¯„ä¼°
   - è®°å½•ç»“æœ

4. **Day 4ï¼šæ¶ˆèå®éªŒ** ğŸ”¬
   - å¯¹æ¯”ä¸åŒæ•°æ®é‡
   - å¯¹æ¯”ä¸åŒæ¶æ„
   - æ’°å†™æŠ¥å‘Š

---

## é¢„æœŸæ€§èƒ½æŒ‡æ ‡

| ä»»åŠ¡ | åŸºçº¿F1 | LoRA F1 | æå‡ |
|------|--------|---------|------|
| FUNSD Entity | 35% | 50-60% | +15-25% |
| FUNSD Relation | 15% | 35-45% | +20-30% |
| DocRED | 25% | 40-50% | +15-25% |
| CORD | 40% | 60-70% | +20-30% |
| FinER | 45% | 65-75% | +20-30% |

*å®é™…ç»“æœå–å†³äºæ•°æ®è´¨é‡ã€æ¨¡å‹é…ç½®å’Œè®­ç»ƒå‚æ•°*
